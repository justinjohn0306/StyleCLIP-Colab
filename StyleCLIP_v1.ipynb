{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleCLIP_v1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ7qOeMuF_HE"
      },
      "source": [
        "# **Welcome to StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MxghaY8-Jm8",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Check GPU type**\n",
        "#@markdown ### Factory reset runtime if you don't have the desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown V100 = Excellent (*Available only for Colab Pro users*)\n",
        "\n",
        "#@markdown P100 = Very Good (*Available only for Colab Pro users*)\n",
        "\n",
        "#@markdown T4 = Good (*preferred*)\n",
        "\n",
        "#@markdown K80 = Meh \n",
        "\n",
        "#@markdown P4 = (*Not Recommended*) \n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as8u59rVVpCe",
        "cellView": "form"
      },
      "source": [
        "#@title #**Setup (may take a few minutes)**\n",
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'encoder4editing'\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "! pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "\n",
        "!git clone https://github.com/omertov/encoder4editing.git $CODE_DIR\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "% cd\n",
        "% cd /content/\n",
        "\n",
        "! pip install ftfy regex tqdm \n",
        "!pip install git+https://github.com/openai/CLIP.git \n",
        "! git clone https://github.com/orpatashnik/StyleCLIP\n",
        "\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfh8bGA5VjU-"
      },
      "source": [
        "# **Select dataset**\n",
        "If you want to use another dataset, please restart runtime. currently, we only support ffhq."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaZbI-6maJin",
        "cellView": "form"
      },
      "source": [
        "dataset_name='ffhq' #@param ['ffhq'] {allow-input: true}\n",
        "# input dataset name \n",
        "\n",
        "% cd\n",
        "% cd /content/StyleCLIP/global_directions/\n",
        "\n",
        "# input prepare data \n",
        "!python GetCode.py --dataset_name $dataset_name --code_type 'w' \n",
        "!python GetCode.py --dataset_name $dataset_name --code_type 's' \n",
        "!python GetCode.py --dataset_name $dataset_name --code_type 's_mean_std' \n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import copy\n",
        "from gdown import download as drive_download\n",
        "import matplotlib.pyplot as plt\n",
        "from MapTS import GetFs,GetBoundary,GetDt\n",
        "from global_directions.manipulate import Manipulator\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device) \n",
        "\n",
        "M=Manipulator(dataset_name='ffhq') \n",
        "fs3=np.load('./npy/ffhq/fs3.npy')\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "image_path = None\n",
        "img_index = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASawJLXGxteR"
      },
      "source": [
        "# **Upload an Image**\n",
        "This section can be skipped if you prefer to edit generated image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIFt31vex-Do",
        "cellView": "form"
      },
      "source": [
        "#@title **e4e setup**\n",
        "#@ e4e setup \n",
        "!pip install gdown --upgrade\n",
        "from gdown import download as drive_download\n",
        "drive_download(\"https://drive.google.com/uc?id=1DbiswG_CT4vbRxXsrZS8iOdvM9N3KWO2\", \"/content/encoder4editing/e4e_ffhq_encode.pt\", quiet=False)\n",
        "experiment_type = 'ffhq_encode'\n",
        "\n",
        "os.chdir('/content/encoder4editing')\n",
        "\n",
        "EXPERIMENT_ARGS = {\n",
        "        \"model_path\": \"e4e_ffhq_encode.pt\"\n",
        "    }\n",
        "EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "resize_dims = (256, 256)\n",
        "\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "# pprint.pprint(opts)  # Display full options used\n",
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts= Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print('Model successfully loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMtmmnQjOXZ3",
        "cellView": "form"
      },
      "source": [
        "#@title ###Run this cell if you wanna try another image after the first one (P.S. : if you ran \"*View video in browser*\" cell you've gotta run this cell before proceeding again)\n",
        "os.chdir('/content/encoder4editing')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLa1ZrjHyw0b",
        "cellView": "form"
      },
      "source": [
        "#@title **Align your input image** (upload an image inside \"*encoder4editing*\" folder using the file browser on the left hand side)\n",
        "image_path = \"cj.jpg\" #@param {type: \"string\"}\n",
        "original_image = Image.open(image_path)\n",
        "original_image = original_image.convert(\"RGB\")\n",
        "if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !curl -O http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "def run_alignment(image_path):\n",
        "  import dlib\n",
        "  from utils.alignment import align_face\n",
        "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image \n",
        "\n",
        "if experiment_type == \"ffhq_encode\":\n",
        "  input_image = run_alignment(image_path)\n",
        "else:\n",
        "  input_image = original_image\n",
        "\n",
        "input_image.resize(resize_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LrmTUUgzOo7",
        "cellView": "form"
      },
      "source": [
        "#@title **Invert the image**\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)\n",
        "\n",
        "def display_alongside_source_image(result_image, source_image):\n",
        "    res = np.concatenate([np.array(source_image.resize(resize_dims)),\n",
        "                          np.array(result_image.resize(resize_dims))], axis=1)\n",
        "    return Image.fromarray(res)\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    if experiment_type == 'cars_encode':\n",
        "        images = images[:, :, 32:224, :]\n",
        "    return images, latents\n",
        "\n",
        "with torch.no_grad():\n",
        "    images, latents = run_on_batch(transformed_image.unsqueeze(0), net)\n",
        "    result_image, latent = images[0], latents[0]\n",
        "torch.save(latents, 'latents.pt')\n",
        "\n",
        "# Display inversion:\n",
        "display_alongside_source_image(tensor2im(result_image), input_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EGeTz0fW_x-"
      },
      "source": [
        "# **Choose Image Index**\n",
        "Relevant only when editing generated image\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "e5tFXAyT0ALd"
      },
      "source": [
        "img_index =  1#@param {type:\"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSqgBTPk54aX"
      },
      "source": [
        "# **Choose Modes** *(and show input image)*\n",
        "Run for both real and generated image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fhnU7Lnq2Tj",
        "cellView": "form"
      },
      "source": [
        "mode='real image' #@param ['real image', 'generated image']\n",
        "\n",
        "if mode == 'real image':\n",
        "  img_index = 0\n",
        "  latents=torch.load('/content/encoder4editing/latents.pt')\n",
        "  w_plus=latents.cpu().detach().numpy()\n",
        "  dlatents_loaded=M.W2S(w_plus)\n",
        "\n",
        "  img_indexs=[img_index]\n",
        "  dlatent_tmp=[tmp[img_indexs] for tmp in dlatents_loaded]\n",
        "elif mode == 'generated image':\n",
        "  img_indexs=[img_index]\n",
        "  dlatent_tmp=[tmp[img_indexs] for tmp in M.dlatents]\n",
        "M.num_images=len(img_indexs)\n",
        "\n",
        "M.alpha=[0]\n",
        "M.manipulate_layers=[0]\n",
        "codes,out=M.EditOneC(0,dlatent_tmp) \n",
        "original=Image.fromarray(out[0,0]).resize((512,512))\n",
        "M.manipulate_layers=None\n",
        "original\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_KnQpoqX28S"
      },
      "source": [
        "# **Input text description** \n",
        "For example\n",
        "\n",
        "| Edit  | Neutral Text | Target Text |\n",
        "| --- | --- | --- |\n",
        "| Smile  | face  | smiling face |\n",
        "| Gender  | female face  | male face |\n",
        "| Blonde hair | face with hair | face with blonde hair |\n",
        "| Hi-top fade | face with hair | face with Hi-top fade hair |\n",
        "| Blue eyes | face with eyes | face with blue eyes |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZMp5pLPrCAh",
        "cellView": "form"
      },
      "source": [
        "neutral='face with eyes' #@param {type:\"string\"}\n",
        "target='face with blue eyes' #@param {type:\"string\"}\n",
        "classnames=[target,neutral]\n",
        "dt=GetDt(classnames,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpmdEls9gHMw"
      },
      "source": [
        "# **Modify manipulation strength *(alhpa)* and disentangle threshold *(beta)***\n",
        "\n",
        "- *Manipulation strength* - positive values correspond to moving along the target direction.\n",
        "- *Disentanglement threshold* - large value means more disentangled edit, just a few channels will be manipulated so only the target attribute will change (for example, grey hair). Small value means less disentangled edit, a large number of channels will be manipulated, related attributes will also change (such as wrinkle, skin color, glasses).\n",
        "\n",
        "- In the terminal, for every manipulation, the number of channels being manipulated is printed (the number is controlled by the attribute (neutral, target) and the disentanglement threshold).\n",
        "\n",
        "- For color transformation, usually 10-20 channels is enough. For large structure change (for example, Hi-top fade), usually 100-200 channels are required.\n",
        "- For an attribute (neutral, target), if you give a low disentanglement threshold, there are just few channels (<20) being manipulated, and usually it is not enough for performing the desired edit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deFVuu4drKHp",
        "cellView": "form"
      },
      "source": [
        "\n",
        "#beta=0.1\n",
        "#alpha=1\n",
        "beta = 0.15 #@param {type:\"slider\", min:0.08, max:0.3, step:0.01}\n",
        "alpha = 4.1 #@param {type:\"slider\", min:-10, max:10, step:0.1}\n",
        "M.alpha=[alpha]\n",
        "boundary_tmp2,c=GetBoundary(fs3,dt,M,threshold=beta)\n",
        "codes=M.MSCode(dlatent_tmp,boundary_tmp2)\n",
        "out=M.GenerateImg(codes)\n",
        "generated=Image.fromarray(out[0,0])#.resize((512,512))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,7), dpi= 100)\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(original)\n",
        "plt.title('original')\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(generated)\n",
        "plt.title('manipulated')\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "cellView": "form",
        "id": "tZuEPlbHSZi5"
      },
      "source": [
        "#@title generate a high-res manipulated photo for download\n",
        "generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "v8uSp4XvSZi6"
      },
      "source": [
        "# **Generate a video**\n",
        "\n",
        "Renders a video interpolating from the base image with provided `beta` to the `target_alpha`. (`target_alpha` can be positive or negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wEUubF9iSZi7",
        "cellView": "form"
      },
      "source": [
        "from contextlib import contextmanager\n",
        "import sys, os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "beta =  0.15 #@param {type:\"number\"}\n",
        "target_alpha =  4.1#@param {type:\"number\"}\n",
        "num_frames = 350 #@param {type:\"number\"}\n",
        "frame_rate = 60 #@param {type:\"number\"}\n",
        "\n",
        "!rm -rf /content/results\n",
        "!mkdir /content/results\n",
        "\n",
        "# Mute GetBoundary()\n",
        "# https://stackoverflow.com/a/25061573\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try:  \n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "def gen_image(beta, alpha, i):\n",
        "  M.alpha=[alpha]\n",
        "  with suppress_stdout():\n",
        "    boundary_tmp2,c=GetBoundary(fs3,dt,M,threshold=beta)\n",
        "  codes=M.MSCode(dlatent_tmp,boundary_tmp2)\n",
        "  out=M.GenerateImg(codes)\n",
        "  Image.fromarray(out[0,0]).save(f\"/content/results/{i:04d}.png\")\n",
        "\n",
        "alphas = np.linspace(0, target_alpha, num_frames)\n",
        "\n",
        "print(\"Generating Frames:\")\n",
        "for i, alpha in tqdm(enumerate(alphas), total=num_frames):\n",
        "  gen_image(beta, alpha, i)\n",
        "\n",
        "print(\"Rendering Video...\")\n",
        "result = os.system(f\"ffmpeg -y -r {frame_rate} -i /content/results/%04d.png -c:v libx264 -vf fps={frame_rate} -pix_fmt yuv420p /content/styleclip.mp4\")\n",
        "print(\"Video saved at styleclip.mp4!\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/styleclip.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "calzEocBIcy6",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**View video in browser**\n",
        "\n",
        "%cd /content/\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('styleclip.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NssJFnHKOC5",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Download the result video** (just in case)\n",
        "from google.colab import files\n",
        "files.download(\"styleclip.mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf-R9FH2H4ek"
      },
      "source": [
        "JS to prevent idle timeout:\n",
        "\n",
        "Press F12 OR CTRL + SHIFT + I OR right click on this website -> inspect.\n",
        "Then click on the console tab and paste in the following code.\n",
        "\n",
        "```javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ]
    }
  ]
}